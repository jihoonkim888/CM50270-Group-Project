{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jihoonkim888/CM50270-Group-Project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f5d7c5a",
        "outputId": "253ca5ec-4d84-4c3f-ba4b-3123197efcf4"
      },
      "source": [
        "# dependencies\n",
        "!pip install gym box2d "
      ],
      "id": "7f5d7c5a",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.7/dist-packages (2.3.10)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58e516fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2988ea47-9601-4e1c-bd19-ad664471c048"
      },
      "source": [
        "# import os \n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
        "!nvidia-smi"
      ],
      "id": "58e516fb",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May  1 18:02:12 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    33W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fa7a298"
      },
      "source": [
        "import time\n",
        "import random\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from replay_buffer import ReplayBuffer"
      ],
      "id": "3fa7a298",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "824b8466"
      },
      "source": [
        "# class ReplayBuffer:\n",
        "    \n",
        "#     def __init__(self, size, input_shape):\n",
        "#         self.size = size\n",
        "#         self.counter = 0\n",
        "#         self.state_buffer = np.zeros((self.size, input_shape), dtype=float)\n",
        "#         self.action_buffer = np.zeros(self.size, dtype=int)\n",
        "#         self.reward_buffer = np.zeros(self.size, dtype=float)\n",
        "#         self.new_state_buffer = np.zeros((self.size, input_shape), dtype=float)\n",
        "#         self.terminal_buffer = np.zeros(self.size, dtype=bool)\n",
        "\n",
        "    \n",
        "#     def store_tuples(self, state, action, reward, new_state, done):\n",
        "#         i = self.counter % self.size\n",
        "#         self.state_buffer[i] = state\n",
        "#         self.action_buffer[i] = action\n",
        "#         self.reward_buffer[i] = reward\n",
        "#         self.new_state_buffer[i] = new_state\n",
        "#         self.terminal_buffer[i] = done\n",
        "#         self.counter += 1\n",
        "\n",
        "    \n",
        "#     def sample_buffer(self, batch_size):\n",
        "#         max_buffer = min(self.counter, self.size)\n",
        "#         batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
        "#         state_batch = self.state_buffer[batch]\n",
        "#         action_batch = self.action_buffer[batch]\n",
        "#         reward_batch = self.reward_buffer[batch]\n",
        "#         new_state_batch = self.new_state_buffer[batch]\n",
        "#         done_batch = self.terminal_buffer[batch]\n",
        "\n",
        "#         return state_batch, action_batch, reward_batch, new_state_batch, done_batch"
      ],
      "id": "824b8466",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11ece5c4"
      },
      "source": [
        "def model(lr, num_actions, input_dims):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(512, input_dim=input_dims, activation='relu'))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(num_actions, activation='linear'))\n",
        "    model.compile(loss='mse',optimizer=Adam(lr=lr))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    return model"
      ],
      "id": "11ece5c4",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7addc488"
      },
      "source": [
        "def plot_graph(episodes, scores, avg_scores, obj):\n",
        "    df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
        "\n",
        "    plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
        "    plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
        "             label='AverageScore')\n",
        "    plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
        "             label='Solved Requirement')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "id": "7addc488",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "similar-volume"
      },
      "source": [
        "class Agent:\n",
        "    \n",
        "    def __init__(self, lr, gamma, epsilon, epsilon_decay, batch_size):\n",
        "        input_dims = 8\n",
        "        num_actions = 4\n",
        "        self.action_space = [i for i in range(num_actions)]\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = 0.01\n",
        "        self.update_rate = 120\n",
        "        self.step_counter = 0\n",
        "\n",
        "        # self.buffer = ReplayBuffer(500000, input_dims)\n",
        "        self.buffer = deque(maxlen=500000)\n",
        "        self.model = model(lr, num_actions, input_dims)\n",
        "        self.target_model = model(lr, num_actions, input_dims)\n",
        "\n",
        "    \n",
        "    # def store_tuple(self, state, action, reward, new_state, done):\n",
        "    #     self.buffer.store_tuples(state, action, reward, new_state, done)\n",
        "\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = np.random.choice(self.action_space)\n",
        "        else:\n",
        "            # state = np.array([observation])\n",
        "            # print(state.shape)\n",
        "            actions = self.model.predict(state)\n",
        "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
        "\n",
        "        return action\n",
        "\n",
        "    \n",
        "    def train(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        if self.step_counter % self.update_rate == 0:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        random_sample = random.sample(self.buffer, self.batch_size)\n",
        "\n",
        "        state_batch = np.squeeze(np.array([i[0] for i in random_sample]))\n",
        "        action_batch = np.array([i[1] for i in random_sample])\n",
        "        reward_batch = np.array([i[2] for i in random_sample])\n",
        "        next_state_batch = np.squeeze(np.array([i[3] for i in random_sample]))\n",
        "        done_batch = np.array([i[4] for i in random_sample])\n",
        "\n",
        "        q_predicted = self.model.predict_on_batch(state_batch)\n",
        "        q_next = self.target_model.predict_on_batch(next_state_batch)\n",
        "        q_max_next = tf.math.reduce_max(q_next, axis=1, keepdims=True).numpy()\n",
        "        q_target = np.copy(q_predicted)\n",
        "\n",
        "        # for i in range(done_batch.shape[0]):\n",
        "        #     target_q_val = reward_batch[i]\n",
        "        #     if not done_batch[i]:\n",
        "        #         target_q_val += self.gamma * q_max_next[i]\n",
        "        #     q_target[i, action_batch[i]] = target_q_val\n",
        "        # self.model.train_on_batch(state_batch, q_target)\n",
        "        # self.step_counter += 1\n",
        "\n",
        "\n",
        "        target_q_val = reward_batch + self.gamma * \\\n",
        "        (np.amax(self.model.predict_on_batch(next_state_batch), axis=1)) * (1 - done_batch)\n",
        "        q_target = self.model.predict_on_batch(state_batch)\n",
        "        indices = np.array([i for i in range(self.batch_size)])\n",
        "        # print(\"target_q_val.shape, q_target.shape, indices.shape, action_batch.shape:\")\n",
        "        # print(target_q_val.shape, q_target.shape, indices.shape, action_batch.shape)\n",
        "        q_target[[indices], [action_batch]] = target_q_val\n",
        "\n",
        "    def train_model(self, env, num_episodes, graph):\n",
        "        \n",
        "        scores, episodes, avg_scores, obj = [], [], [], []\n",
        "        goal = 150\n",
        "        avg_score = 0\n",
        "        f = 0\n",
        "        txt = open(\"saved_networks.txt\", \"w\")\n",
        "        t1 = time.perf_counter()\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "\n",
        "            # Early stopping...\n",
        "            if avg_score > goal:\n",
        "                print(\"The average rewards of the last 100 episodes > {}. Early stopping in Episode {}...\".format(goal, i))\n",
        "                self.model.save((\"saved_networks/dqn_model{0}\".format(i)))\n",
        "                self.model.save_weights((\"saved_networks/dqn_model{0}/net_weights{0}.h5\".format(i)))\n",
        "                txt.write(\"Save {0} - Episode {1}/{2}, Score: {3} ({4}), AVG Score: {5}\\n\".format(i, i, num_episodes,\n",
        "                                                                                                  score, self.epsilon,\n",
        "                                                                                                  avg_score))\n",
        "                return\n",
        "\n",
        "            done = False\n",
        "            score = 0.0\n",
        "            state = env.reset()\n",
        "            while not done:\n",
        "                # print('state:', state)\n",
        "                state = state.reshape(1,-1)\n",
        "                action = self.get_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                score += reward\n",
        "                self.buffer.append((state, action, reward, next_state, done))\n",
        "                state = next_state\n",
        "                self.train()\n",
        "            scores.append(score)\n",
        "            obj.append(goal)\n",
        "            episodes.append(i)\n",
        "            avg_score = np.mean(scores[-100:])\n",
        "            avg_scores.append(avg_score)\n",
        "            \n",
        "            # avg_score_10 = np.mean(scores[-10:])\n",
        "            \n",
        "            print_count = 50\n",
        "            if (i % print_count == 0) and (i != 0):\n",
        "#                 plot_graph(episodes, scores, avg_scores, obj)\n",
        "                print(\"Episode {0}/{1}, Score: {2} ({3}), AVG Score: {4}\".format(i, num_episodes, score, self.epsilon, avg_score))\n",
        "                t2 = time.perf_counter()\n",
        "                print(\"Finished {} episodes in {} seconds\".format(print_count, t2-t1))\n",
        "                t1 = time.perf_counter()\n",
        "                \n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n",
        "            \n",
        "\n",
        "            \n",
        "            if (i==0) or (i==num_episodes-1):\n",
        "                self.model.save((\"saved_networks/dqn_model{0}\".format(i)))\n",
        "                self.model.save_weights((\"saved_networks/dqn_model{0}/net_weights{0}.h5\".format(i)))\n",
        "                txt.write(\"Save {0} - Episode {1}/{2}, Score: {3} ({4}), AVG Score: {5}\\n\".format(i, i, num_episodes,\n",
        "                                                                                                  score, self.epsilon,\n",
        "                                                                                                  avg_score))\n",
        "#                 f += 1\n",
        "                print(\"Network saved\")\n",
        "\n",
        "        txt.close()\n",
        "        \n",
        "        if graph:\n",
        "            \n",
        "            plot_graph(episodes, scores, avg_scores, obj)\n",
        "#             df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
        "\n",
        "#             plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
        "#             plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
        "#                      label='AverageScore')\n",
        "#             plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
        "#                      label='Solved Requirement')\n",
        "#             plt.legend()\n",
        "#             plt.savefig('LunarLander_Train.png')\n",
        "            \n",
        "        return scores\n",
        "\n",
        "    def test(self, env, num_episodes, file_type, file, graph):\n",
        "        if file_type == 'tf':\n",
        "            self.model = tf.keras.models.load_model(file)\n",
        "        elif file_type == 'h5':\n",
        "            self.train_model(env, 5, False)\n",
        "            self.model.load_weights(file)\n",
        "        self.epsilon = 0.0\n",
        "        scores, episodes, avg_scores, obj = [], [], [], []\n",
        "        goal = 200\n",
        "        score = 0.0\n",
        "        for i in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            episode_score = 0.0\n",
        "            while not done:\n",
        "                action = self.get_action(state)\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                episode_score += reward\n",
        "                state = next_state\n",
        "            score += episode_score\n",
        "            scores.append(episode_score)\n",
        "            print(f\"{i}th round - {episode_score}\")\n",
        "            obj.append(goal)\n",
        "            episodes.append(i)\n",
        "            avg_score = np.mean(scores[-100:])\n",
        "            avg_scores.append(avg_score)\n",
        "\n",
        "        if graph:\n",
        "            df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
        "\n",
        "            plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
        "            plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
        "                     label='AverageScore')\n",
        "            plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
        "                     label='Solved Requirement')\n",
        "            plt.legend()\n",
        "            plt.savefig('LunarLander_Test.png')\n",
        "\n",
        "        env.close()\n",
        "\n"
      ],
      "id": "similar-volume",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25a4354f",
        "outputId": "da762806-7301-44ee-97b9-0b040c70b313"
      },
      "source": [
        "dqn_agent = Agent(lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, batch_size=64)"
      ],
      "id": "25a4354f",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 136,964\n",
            "Trainable params: 136,964\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 512)               4608      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 136,964\n",
            "Trainable params: 136,964\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "522378a6"
      },
      "source": [
        "env = gym.make(\"LunarLander-v2\")\n",
        "spec = gym.spec(\"LunarLander-v2\")\n",
        "train = 1\n",
        "test = 0\n",
        "num_episodes = 5000\n",
        "graph = True\n",
        "\n",
        "file_type = 'h5'\n",
        "file = 'saved_networks/dqn_model0'"
      ],
      "id": "522378a6",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "c11acd82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3e0a1e1-2d03-4473-8306-7b4886d811f4"
      },
      "source": [
        "t_start = time.perf_counter()\n",
        "\n",
        "\n",
        "if train and not test:\n",
        "    scores = dqn_agent.train_model(env, num_episodes, graph)\n",
        "else:\n",
        "    dqn_agent.test(env, num_episodes, file_type, file, graph)\n",
        "    \n",
        "t_end = time.perf_counter()\n",
        "\n",
        "print(f\"Finished in {t_end-t_start} seconds\")"
      ],
      "id": "c11acd82",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_networks/dqn_model0/assets\n",
            "Network saved\n",
            "Episode 50/5000, Score: -136.49573011484188 (0.778312557068642), AVG Score: -165.06473148724703\n",
            "Finished 50 episodes in 119.77066469300007 seconds\n",
            "Episode 100/5000, Score: -129.4115072713381 (0.6057704364907278), AVG Score: -170.4467411679738\n",
            "Finished 50 episodes in 154.55134402600015 seconds\n",
            "Episode 150/5000, Score: -187.58755173832498 (0.47147873742168567), AVG Score: -154.25483454118424\n",
            "Finished 50 episodes in 165.85285007399943 seconds\n",
            "Episode 200/5000, Score: -175.89782493494283 (0.3669578217261671), AVG Score: -137.97936127971306\n",
            "Finished 50 episodes in 175.5595439259996 seconds\n",
            "Episode 250/5000, Score: -155.56021372022718 (0.285607880564032), AVG Score: -139.80076546010375\n",
            "Finished 50 episodes in 189.76268373999937 seconds\n",
            "Episode 300/5000, Score: -79.0118810087958 (0.22229219984074702), AVG Score: -136.9478453440793\n",
            "Finished 50 episodes in 206.85230608599977 seconds\n",
            "Episode 350/5000, Score: -112.64769070626598 (0.1730128104744653), AVG Score: -137.19680973790662\n",
            "Finished 50 episodes in 200.8070662760001 seconds\n",
            "Episode 400/5000, Score: -126.80930443008987 (0.1346580429260134), AVG Score: -131.67806945308487\n",
            "Finished 50 episodes in 193.20002189499974 seconds\n",
            "Episode 450/5000, Score: -283.01427731356864 (0.10480604571960442), AVG Score: -129.76126052332057\n",
            "Finished 50 episodes in 214.5391850189999 seconds\n",
            "Episode 500/5000, Score: -132.3152442799776 (0.08157186144027828), AVG Score: -133.12282878411654\n",
            "Finished 50 episodes in 203.00217538499965 seconds\n",
            "Episode 550/5000, Score: -120.34447694625466 (0.06348840406243188), AVG Score: -139.41928489233956\n",
            "Finished 50 episodes in 210.90816728400023 seconds\n",
            "Episode 600/5000, Score: -109.18705807274897 (0.0494138221100385), AVG Score: -134.94264943564622\n",
            "Finished 50 episodes in 211.66218793000007 seconds\n",
            "Episode 650/5000, Score: -148.90875840441436 (0.03845939824099909), AVG Score: -124.65409834533415\n",
            "Finished 50 episodes in 210.70452980999926 seconds\n",
            "Episode 700/5000, Score: -86.19505335572198 (0.029933432588273214), AVG Score: -126.23918471237174\n",
            "Finished 50 episodes in 215.96093327900053 seconds\n",
            "Episode 750/5000, Score: -150.46020550907934 (0.023297566459620722), AVG Score: -125.02623219495085\n",
            "Finished 50 episodes in 216.210870678 seconds\n",
            "Episode 800/5000, Score: -118.57523165548852 (0.018132788524664028), AVG Score: -129.02890472641818\n",
            "Finished 50 episodes in 219.6483929369997 seconds\n",
            "Episode 850/5000, Score: -145.19371951351178 (0.014112977003416188), AVG Score: -132.40786230154973\n",
            "Finished 50 episodes in 227.18911341200055 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6322f7d"
      },
      "source": [
        "np.savetxt('scores.out', np.array(scores), delimiter=',') # saving scores for each episode to scores.out"
      ],
      "id": "d6322f7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne1s_J1Z1pQO"
      },
      "source": [
        ""
      ],
      "id": "Ne1s_J1Z1pQO",
      "execution_count": null,
      "outputs": []
    }
  ]
}