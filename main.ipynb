{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1f53f8f",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jihoonkim888/CM50270-Group-Project/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5d7c5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f5d7c5a",
    "outputId": "253ca5ec-4d84-4c3f-ba4b-3123197efcf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (0.18.0)\r\n",
      "Requirement already satisfied: box2d in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (2.3.10)\r\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (from gym) (1.20.1)\r\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (from gym) (7.2.0)\r\n",
      "Requirement already satisfied: scipy in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (from gym) (1.6.2)\r\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (from gym) (1.6.0)\r\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (from gym) (1.5.0)\r\n",
      "Requirement already satisfied: future in /home/jihoon/anaconda3/envs/tf38/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\r\n"
     ]
    }
   ],
   "source": [
    "# dependencies\n",
    "!pip install gym box2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e516fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58e516fb",
    "outputId": "2988ea47-9601-4e1c-bd19-ad664471c048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  2 04:29:11 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.119.03   Driver Version: 450.119.03   CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 970     Off  | 00000000:06:00.0 Off |                  N/A |\r\n",
      "| 49%   37C    P0    48W / 250W |     59MiB /  4039MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A       942      G   /usr/lib/xorg/Xorg                 47MiB |\r\n",
      "|    0   N/A  N/A      1141      G   /usr/bin/gnome-shell                7MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# import os \n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa7a298",
   "metadata": {
    "id": "3fa7a298"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824b8466",
   "metadata": {
    "id": "824b8466"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, size, input_shape):\n",
    "        self.size = size\n",
    "        self.counter = 0\n",
    "        self.state_buffer = np.zeros((self.size, input_shape), dtype=float)\n",
    "        self.action_buffer = np.zeros(self.size, dtype=int)\n",
    "        self.reward_buffer = np.zeros(self.size, dtype=float)\n",
    "        self.next_state_buffer = np.zeros((self.size, input_shape), dtype=float)\n",
    "        self.terminal_buffer = np.zeros(self.size, dtype=bool)\n",
    "\n",
    "    \n",
    "    def store_tuples(self, state, action, reward, next_state, done):\n",
    "        i = self.counter % self.size\n",
    "        self.state_buffer[i] = state\n",
    "        self.action_buffer[i] = action\n",
    "        self.reward_buffer[i] = reward\n",
    "        self.next_state_buffer[i] = next_state\n",
    "        self.terminal_buffer[i] = done\n",
    "        self.counter += 1\n",
    "\n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_buffer = min(self.counter, self.size)\n",
    "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
    "        state_batch = self.state_buffer[batch]\n",
    "        action_batch = self.action_buffer[batch]\n",
    "        reward_batch = self.reward_buffer[batch]\n",
    "        next_state_batch = self.next_state_buffer[batch]\n",
    "        done_batch = self.terminal_buffer[batch]\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ece5c4",
   "metadata": {
    "id": "11ece5c4"
   },
   "outputs": [],
   "source": [
    "def model(lr, num_actions, input_dims):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_dim=input_dims, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=Adam(lr=lr))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7addc488",
   "metadata": {
    "id": "7addc488"
   },
   "outputs": [],
   "source": [
    "def plot_graph(episodes, scores, avg_scores, obj):\n",
    "    df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
    "\n",
    "    plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
    "    plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
    "             label='AverageScore')\n",
    "    plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
    "             label='Solved Requirement')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "similar-volume",
   "metadata": {
    "id": "similar-volume"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, lr, gamma, epsilon, epsilon_decay, batch_size):\n",
    "        input_dims = 8\n",
    "        num_actions = 4\n",
    "        self.action_space = [i for i in range(num_actions)]\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.01\n",
    "        self.update_rate = 120\n",
    "        self.step_counter = 0\n",
    "\n",
    "        self.buffer = ReplayBuffer(500000, input_dims)\n",
    "#         self.buffer = deque(maxlen=500000)\n",
    "        self.model = model(lr, num_actions, input_dims)\n",
    "        self.target_model = model(lr, num_actions, input_dims)\n",
    "\n",
    "    \n",
    "    def store_tuple(self, state, action, reward, next_state, done):\n",
    "        self.buffer.store_tuples(state, action, reward, next_state, done)\n",
    "\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            # print(state.shape)\n",
    "            actions = self.model.predict(np.array(state))\n",
    "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        if self.buffer.counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        if self.step_counter % self.update_rate == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "#         random_sample = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = \\\n",
    "            self.buffer.sample_buffer(self.batch_size)\n",
    "\n",
    "        q_predicted = self.model(state_batch)\n",
    "        q_next = self.target_model(next_state_batch)\n",
    "        q_max_next = tf.math.reduce_max(q_next, axis=1, keepdims=True).numpy()\n",
    "        q_target = np.copy(q_predicted)\n",
    "\n",
    "        for i in range(done_batch.shape[0]):\n",
    "            target_q_val = reward_batch[i]\n",
    "            if not done_batch[i]:\n",
    "                target_q_val += self.gamma * q_max_next[i]\n",
    "            q_target[i, action_batch[i]] = target_q_val\n",
    "        self.model.train_on_batch(state_batch, q_target)\n",
    "        self.step_counter += 1\n",
    "\n",
    "\n",
    "#         target_q_val = reward_batch + self.gamma * \\\n",
    "#         (np.amax(self.model.predict_on_batch(next_state_batch), axis=1)) * (1 - done_batch)\n",
    "#         q_target = self.model.predict_on_batch(state_batch)\n",
    "#         indices = np.array([i for i in range(self.batch_size)])\n",
    "#         # print(\"target_q_val.shape, q_target.shape, indices.shape, action_batch.shape:\")\n",
    "#         # print(target_q_val.shape, q_target.shape, indices.shape, action_batch.shape)\n",
    "#         q_target[[indices], [action_batch]] = target_q_val\n",
    "\n",
    "    def train_model(self, env, num_episodes, graph):\n",
    "        \n",
    "        scores, episodes, avg_scores, obj = [], [], [], []\n",
    "        goal = 150\n",
    "        avg_score = 0\n",
    "        txt = open(\"saved_networks.txt\", \"w\")\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "\n",
    "            # Early stopping...\n",
    "            if avg_score > goal:\n",
    "                print(\"The average rewards of the last 100 episodes > {}. Early stopping in Episode {}...\".format(goal, i))\n",
    "                self.model.save((\"saved_networks/dqn_model{0}\".format(i)))\n",
    "                self.model.save_weights((\"saved_networks/dqn_model{0}/net_weights{0}.h5\".format(i)))\n",
    "                txt.write(\"Save {0} - Episode {1}/{2}, Score: {3} ({4}), AVG Score: {5}\\n\".format(i, i, num_episodes,\n",
    "                                                                                                  score, self.epsilon,\n",
    "                                                                                                  avg_score))\n",
    "                return\n",
    "\n",
    "            done = False\n",
    "            score = 0.0\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                # print('state:', state)\n",
    "                state = state.reshape(1,-1)\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "                self.store_tuple(state, action, reward, next_state, done)\n",
    "#                 self.buffer.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "                self.train()\n",
    "            scores.append(score)\n",
    "            obj.append(goal)\n",
    "            episodes.append(i)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            avg_scores.append(avg_score)\n",
    "            \n",
    "            # avg_score_10 = np.mean(scores[-10:])\n",
    "            \n",
    "            print_count = 50\n",
    "            if (i % print_count == 0) and (i != 0):\n",
    "#                 plot_graph(episodes, scores, avg_scores, obj)\n",
    "                print(\"Episode {0}/{1}, Score: {2} ({3}), AVG Score: {4}\".format(i, num_episodes, score, self.epsilon, avg_score))\n",
    "                t2 = time.perf_counter()\n",
    "                print(\"Finished {} episodes in {} seconds\".format(print_count, t2-t1))\n",
    "                t1 = time.perf_counter()\n",
    "                \n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "\n",
    "            \n",
    "            if (i==0) or (i==num_episodes-1):\n",
    "                self.model.save((\"saved_networks/dqn_model{0}\".format(i)))\n",
    "                self.model.save_weights((\"saved_networks/dqn_model{0}/net_weights{0}.h5\".format(i)))\n",
    "                txt.write(\"Save {0} - Episode {1}/{2}, Score: {3} ({4}), AVG Score: {5}\\n\".format(i, i, num_episodes,\n",
    "                                                                                                  score, self.epsilon,\n",
    "                                                                                                  avg_score))\n",
    "#                 f += 1\n",
    "                print(\"Network saved\")\n",
    "\n",
    "        txt.close()\n",
    "        \n",
    "        if graph:\n",
    "            \n",
    "            plot_graph(episodes, scores, avg_scores, obj)\n",
    "#             df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
    "\n",
    "#             plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
    "#             plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
    "#                      label='AverageScore')\n",
    "#             plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
    "#                      label='Solved Requirement')\n",
    "#             plt.legend()\n",
    "#             plt.savefig('LunarLander_Train.png')\n",
    "            \n",
    "        return scores\n",
    "\n",
    "    def test(self, env, num_episodes, file_type, file, graph):\n",
    "        if file_type == 'tf':\n",
    "            self.model = tf.keras.models.load_model(file)\n",
    "        elif file_type == 'h5':\n",
    "            self.train_model(env, 5, False)\n",
    "            self.model.load_weights(file)\n",
    "        self.epsilon = 0.0\n",
    "        scores, episodes, avg_scores, obj = [], [], [], []\n",
    "        goal = 200\n",
    "        score = 0.0\n",
    "        for i in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_score = 0.0\n",
    "            while not done:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_score += reward\n",
    "                state = next_state\n",
    "            score += episode_score\n",
    "            scores.append(episode_score)\n",
    "            print(f\"{i}th round - {episode_score}\")\n",
    "            obj.append(goal)\n",
    "            episodes.append(i)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            avg_scores.append(avg_score)\n",
    "\n",
    "        if graph:\n",
    "            df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
    "\n",
    "            plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
    "            plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
    "                     label='AverageScore')\n",
    "            plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
    "                     label='Solved Requirement')\n",
    "            plt.legend()\n",
    "            plt.savefig('LunarLander_Test.png')\n",
    "\n",
    "        env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a4354f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25a4354f",
    "outputId": "da762806-7301-44ee-97b9-0b040c70b313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 136,964\n",
      "Trainable params: 136,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 136,964\n",
      "Trainable params: 136,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = Agent(lr=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522378a6",
   "metadata": {
    "id": "522378a6"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "spec = gym.spec(\"LunarLander-v2\")\n",
    "train = 1\n",
    "test = 0\n",
    "num_episodes = 5000\n",
    "graph = True\n",
    "\n",
    "file_type = 'h5'\n",
    "file = 'saved_networks/dqn_model0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11acd82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c11acd82",
    "outputId": "a3e0a1e1-2d03-4473-8306-7b4886d811f4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_networks/dqn_model0/assets\n",
      "Network saved\n",
      "Episode 50/5000, Score: -82.24658800017437 (0.778312557068642), AVG Score: -160.42380164604475\n",
      "Finished 50 episodes in 36.00515631202143 seconds\n"
     ]
    }
   ],
   "source": [
    "t_start = time.perf_counter()\n",
    "\n",
    "\n",
    "if train and not test:\n",
    "    scores = dqn_agent.train_model(env, num_episodes, graph)\n",
    "else:\n",
    "    dqn_agent.test(env, num_episodes, file_type, file, graph)\n",
    "    \n",
    "t_end = time.perf_counter()\n",
    "\n",
    "print(f\"Finished in {t_end-t_start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6322f7d",
   "metadata": {
    "id": "d6322f7d"
   },
   "outputs": [],
   "source": [
    "np.savetxt('scores.out', np.array(scores), delimiter=',') # saving scores for each episode to scores.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ne1s_J1Z1pQO",
   "metadata": {
    "id": "Ne1s_J1Z1pQO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
