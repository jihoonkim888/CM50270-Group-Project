{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f5d7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "# !pip install gym box2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e516fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa7a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824b8466",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, size, input_shape):\n",
    "        self.size = size\n",
    "        self.counter = 0\n",
    "        self.state_buffer = np.zeros((self.size, input_shape), dtype=float)\n",
    "        self.action_buffer = np.zeros(self.size, dtype=int)\n",
    "        self.reward_buffer = np.zeros(self.size, dtype=float)\n",
    "        self.new_state_buffer = np.zeros((self.size, input_shape), dtype=float)\n",
    "        self.terminal_buffer = np.zeros(self.size, dtype=bool)\n",
    "\n",
    "    \n",
    "    def store_tuples(self, state, action, reward, new_state, done):\n",
    "        i = self.counter % self.size\n",
    "        self.state_buffer[i] = state\n",
    "        self.action_buffer[i] = action\n",
    "        self.reward_buffer[i] = reward\n",
    "        self.new_state_buffer[i] = new_state\n",
    "        self.terminal_buffer[i] = done\n",
    "        self.counter += 1\n",
    "\n",
    "    \n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_buffer = min(self.counter, self.size)\n",
    "        batch = np.random.choice(max_buffer, batch_size, replace=False)\n",
    "        state_batch = self.state_buffer[batch]\n",
    "        action_batch = self.action_buffer[batch]\n",
    "        reward_batch = self.reward_buffer[batch]\n",
    "        new_state_batch = self.new_state_buffer[batch]\n",
    "        done_batch = self.terminal_buffer[batch]\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, new_state_batch, done_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ece5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DeepQNetwork(lr, num_actions, input_dims, fc1, fc2):\n",
    "    q_net = Sequential()\n",
    "    q_net.add(Dense(fc1, input_dim=input_dims, activation='relu'))\n",
    "    q_net.add(Dense(fc2, activation='relu'))\n",
    "    q_net.add(Dense(num_actions, activation=None))\n",
    "    q_net.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
    "\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7addc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(episodes, scores, avg_scores, obj):\n",
    "    df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
    "\n",
    "    plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
    "    plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
    "             label='AverageScore')\n",
    "    plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
    "             label='Solved Requirement')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "similar-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, lr, discount_factor, num_actions, epsilon, epsilon_decay, batch_size, input_dims, fc1, fc2):\n",
    "        self.action_space = [i for i in range(num_actions)]\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.update_rate = 120\n",
    "        self.step_counter = 0\n",
    "        self.buffer = ReplayBuffer(500000, input_dims)\n",
    "        self.q_net = DeepQNetwork(lr, num_actions, input_dims, fc1, fc2)\n",
    "        self.q_target_net = DeepQNetwork(lr, num_actions, input_dims, fc1, fc2)\n",
    "\n",
    "    \n",
    "    def store_tuple(self, state, action, reward, new_state, done):\n",
    "        self.buffer.store_tuples(state, action, reward, new_state, done)\n",
    "\n",
    "    \n",
    "    def policy(self, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.action_space)\n",
    "        else:\n",
    "            state = np.array([observation])\n",
    "            actions = self.q_net(state)\n",
    "            action = tf.math.argmax(actions, axis=1).numpy()[0]\n",
    "\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        if self.buffer.counter < self.batch_size:\n",
    "            return\n",
    "        if self.step_counter % self.update_rate == 0:\n",
    "            self.q_target_net.set_weights(self.q_net.get_weights())\n",
    "\n",
    "        state_batch, action_batch, reward_batch, new_state_batch, done_batch = \\\n",
    "            self.buffer.sample_buffer(self.batch_size)\n",
    "\n",
    "        q_predicted = self.q_net(state_batch)\n",
    "        q_next = self.q_target_net(new_state_batch)\n",
    "        q_max_next = tf.math.reduce_max(q_next, axis=1, keepdims=True).numpy()\n",
    "        q_target = np.copy(q_predicted)\n",
    "\n",
    "        for i in range(done_batch.shape[0]):\n",
    "            target_q_val = reward_batch[i]\n",
    "            if not done_batch[i]:\n",
    "                target_q_val += self.discount_factor*q_max_next[i]\n",
    "            q_target[i, action_batch[i]] = target_q_val\n",
    "        self.q_net.train_on_batch(state_batch, q_target)\n",
    "        self.step_counter += 1\n",
    "\n",
    "    def train_model(self, env, num_episodes, graph):\n",
    "        \n",
    "        scores, episodes, avg_scores, obj = [], [], [], []\n",
    "        goal = 200\n",
    "        f = 0\n",
    "        txt = open(\"saved_networks.txt\", \"w\")\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            done = False\n",
    "            score = 0.0\n",
    "            state = env.reset()\n",
    "            while not done:\n",
    "                action = self.policy(state)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                score += reward\n",
    "                self.store_tuple(state, action, reward, new_state, done)\n",
    "                state = new_state\n",
    "                self.train()\n",
    "            scores.append(score)\n",
    "            obj.append(goal)\n",
    "            episodes.append(i)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            avg_scores.append(avg_score)\n",
    "            \n",
    "            avg_score_10 = np.mean(scores[-10:])\n",
    "            \n",
    "            print_count = 100\n",
    "            if (i % print_count == 0) and (i != 0):\n",
    "#                 plot_graph(episodes, scores, avg_scores, obj)\n",
    "                print(\"Episode {0}/{1}, Score: {2} ({3}), AVG Score: {4}\".format(i, num_episodes, score, self.epsilon, avg_score))\n",
    "                t2 = time.perf_counter()\n",
    "                print(\"Finished {} episodes in {} seconds\".format(print_count, t2-t1))\n",
    "                t1 = time.perf_counter()\n",
    "                \n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            if avg_score_10 > goal:\n",
    "                print(\"The average rewards of the last 10 episodes > {}. Early stopping...\".format(goal))\n",
    "                self.q_net.save((\"saved_networks/dqn_model{0}\".format(i)))\n",
    "                self.q_net.save_weights((\"saved_networks/dqn_model{0}/net_weights{0}.h5\".format(i)))\n",
    "                txt.write(\"Save {0} - Episode {1}/{2}, Score: {3} ({4}), AVG Score: {5}\\n\".format(i, i, num_episodes,\n",
    "                                                                                                  score, self.epsilon,\n",
    "                                                                                                  avg_score))\n",
    "                break\n",
    "            \n",
    "            if (i==0) or (i==num_episodes-1):\n",
    "                self.q_net.save((\"saved_networks/dqn_model{0}\".format(i)))\n",
    "                self.q_net.save_weights((\"saved_networks/dqn_model{0}/net_weights{0}.h5\".format(i)))\n",
    "                txt.write(\"Save {0} - Episode {1}/{2}, Score: {3} ({4}), AVG Score: {5}\\n\".format(i, i, num_episodes,\n",
    "                                                                                                  score, self.epsilon,\n",
    "                                                                                                  avg_score))\n",
    "#                 f += 1\n",
    "                print(\"Network saved\")\n",
    "\n",
    "        txt.close()\n",
    "        \n",
    "        if graph:\n",
    "            \n",
    "            plot_graph(episodes, scores, avg_scores, obj)\n",
    "#             df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
    "\n",
    "#             plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
    "#             plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
    "#                      label='AverageScore')\n",
    "#             plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
    "#                      label='Solved Requirement')\n",
    "#             plt.legend()\n",
    "#             plt.savefig('LunarLander_Train.png')\n",
    "            \n",
    "        return scores\n",
    "\n",
    "    def test(self, env, num_episodes, file_type, file, graph):\n",
    "        if file_type == 'tf':\n",
    "            self.q_net = tf.keras.models.load_model(file)\n",
    "        elif file_type == 'h5':\n",
    "            self.train_model(env, 5, False)\n",
    "            self.q_net.load_weights(file)\n",
    "        self.epsilon = 0.0\n",
    "        scores, episodes, avg_scores, obj = [], [], [], []\n",
    "        goal = 200\n",
    "        score = 0.0\n",
    "        for i in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_score = 0.0\n",
    "            while not done:\n",
    "                action = self.policy(state)\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                episode_score += reward\n",
    "                state = new_state\n",
    "            score += episode_score\n",
    "            scores.append(episode_score)\n",
    "            print(f\"{i}th round - {episode_score}\")\n",
    "            obj.append(goal)\n",
    "            episodes.append(i)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            avg_scores.append(avg_score)\n",
    "\n",
    "        if graph:\n",
    "            df = pd.DataFrame({'x': episodes, 'Score': scores, 'Average Score': avg_scores, 'Solved Requirement': obj})\n",
    "\n",
    "            plt.plot('x', 'Score', data=df, marker='', color='blue', linewidth=2, label='Score')\n",
    "            plt.plot('x', 'Average Score', data=df, marker='', color='orange', linewidth=2, linestyle='dashed',\n",
    "                     label='AverageScore')\n",
    "            plt.plot('x', 'Solved Requirement', data=df, marker='', color='red', linewidth=2, linestyle='dashed',\n",
    "                     label='Solved Requirement')\n",
    "            plt.legend()\n",
    "            plt.savefig('LunarLander_Test.png')\n",
    "\n",
    "        env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a4354f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               4608      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 136,964\n",
      "Trainable params: 136,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = Agent(lr=0.001, discount_factor=0.99, num_actions=4, epsilon=1.0, epsilon_decay=0.995, batch_size=128, input_dims=8, fc1=512, fc2=256)\n",
    "dqn_agent.q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522378a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "spec = gym.spec(\"LunarLander-v2\")\n",
    "train = 1\n",
    "test = 0\n",
    "num_episodes = 5000\n",
    "graph = True\n",
    "\n",
    "file_type = 'h5'\n",
    "file = 'saved_networks/dqn_model0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11acd82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_networks/dqn_model0/assets\n",
      "Network saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-50b8fdd51653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdqn_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-069e3fa9758b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, env, num_episodes, graph)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-069e3fa9758b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mtarget_q_val\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mq_max_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mq_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_q_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1721\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[0m\u001b[1;32m   1724\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                                     class_weight)\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    395\u001b[0m           (options.experimental_optimization.apply_default_optimizations  # pylint: disable=g-bool-id-comparison\n\u001b[1;32m    396\u001b[0m            is not False)):\n\u001b[0;32m--> 397\u001b[0;31m       dataset = _OptimizeDataset(dataset, graph_rewrites.enabled,\n\u001b[0m\u001b[1;32m    398\u001b[0m                                  \u001b[0mgraph_rewrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                                  graph_rewrites.default, graph_rewrite_configs)\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, optimization_configs)\u001b[0m\n\u001b[1;32m   4575\u001b[0m         argument_dtype=dtypes.string)\n\u001b[1;32m   4576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4577\u001b[0;31m     variant_tensor = gen_dataset_ops.optimize_dataset_v2(\n\u001b[0m\u001b[1;32m   4578\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizations_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf38/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset_v2\u001b[0;34m(input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[1;32m   4019\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4021\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   4022\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"OptimizeDatasetV2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations_enabled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4023\u001b[0m         \u001b[0moptimizations_disabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizations_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_start = time.perf_counter()\n",
    "\n",
    "\n",
    "if train and not test:\n",
    "    scores = dqn_agent.train_model(env, num_episodes, graph)\n",
    "else:\n",
    "    dqn_agent.test(env, num_episodes, file_type, file, graph)\n",
    "    \n",
    "t_end = time.perf_counter()\n",
    "\n",
    "print(\"Finished {} episodes in {} seconds\".format(num_episodes, t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6322f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('scores.out', np.array(scores), delimiter=',') # saving scores for each episode to scores.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c038f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
